{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 정제하기\n",
    "가장 기본적인 정제 방법은 strip, replace, split와 같은 파이썬의 기본 문자열 메서드를 사용하여 텍스트를 바꾸는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"   Interrobang. By Aishwarya Henriette    \",\n",
    "             \"Parking And Going. By Karl Gautier\", \n",
    "             \"    Today Is The night. By Jarek Prakash  \"]\n",
    "\n",
    "# 공백 문자를 제거합니다.\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마침표를 제거합니다.\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 정의 변환 함수를 정의하고 적용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 정규 표현식을 사용하여 강력한 문자열 치환을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 텍스트 데이터는 특성으로 만들기 전에 정제되어야 합니다. 파이썬의 문자열 연산을 사용하는 것이 가장 기본적인 텍스트 정제 방법입니다. 실전에서는 대부분 사용자 정의 함수(예를 들면 capitalizer)를 정의하여 다른 정제 작업과 연결하고 이를 텍스트 데이터에 적용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 HTML 파싱과 정제하기\n",
    "beautiful soup의 다양한 기능을 사용하여 HTML을 파싱할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\dain\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예제 HTML 코드를 만듭니다.\n",
    "html = \"\"\"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "\"\"\"\n",
    "\n",
    "# html을 파싱합니다.\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# \"full_name\" 이름의 클래스를 가진 div를 찾아 텍스트를 출력합니다.\n",
    "soup.find(\"div\", {\"class\" : \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름이 이상하지만 뷰티풀 수프는 HTML 스크래핑을 위한 강력한 파이썬 라이브러리입니다. 일반적으로 뷰티풀 수프는 웹사이트를 스크래핑하는 데 사용합니다. 하지만 HTML에 들어 있는 텍스트 데이터를 추출하는 데 사용할 수도 있습니다. 이 해결에서 소개하는 몇 개의 메서드만으로도 HTML 코드를 파싱하여 원하는 데이터를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 구두점 삭제하기\n",
    "구두점 글자의 딕셔너리를 만들어 translate 메서드에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '1000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '1000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "# 구두점 문자로 이루어진 딕셔너리를 만듭니다.\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# 문자열의 구두점을 삭제합니다.\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate는 속도가 매우 빨라 인기 있는 파이썬 함수입니다. 이 해결은 모든 유니코드 구두점을 키로 하고 값은 None인 punctuation 딕셔너리를 만들었습니다. 그 다음 문자열에서 punctuation에 있는 모든 문자를 None으로 바꾸어 구두점을 삭제하는 효과를 냅니다. 조금 더 읽기 좋은 코드를 사용하는 방법도 있지만 이 방식이 다른 것보다 훨씬 더 빠르다는 장점이 있습니다. \n",
    "\n",
    "구두점도 정보가 있다는 사실을 유념하세요(예를 들어 \"맞아?\"와 \"맞아!\"). 구두점을 삭제하는 것은 특성을 만드는 데 필요악일 경우가 많습니다. 하지만 구두점이 중요한 역할을 한다면 이를 고려해야만 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 텍스트 토큰화하기\n",
    "파이썬의 자연어 처리 툴킷인 NLTK(natural language toolkit)는 단어 토큰화를 비롯해 강력한 텍스트 처리 기능을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dain\\anaconda3\\lib\\site-packages (3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# 단어를 토큰으로 나눕니다.\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장으로 나눌 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화, 특히 단어 토큰화는 텍스트 데이터를 정제한 후 빈번하게 수행하는 작업입니다. 이는 유용한 특성을 만들기 위해 텍스트를 데이터로 변환하는 첫 번째 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 불용어 삭제하기\n",
    "NLTK의 stopwords를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['going', 'store', 'park']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "\n",
    "# 불용어를 적재합니다.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 불용어를 삭제합니다.\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어는 작업 전에 삭제해야 하는 일련의 단어를 의미하기도 하지만 종종 유용한 정보가 거의 없는 매우 자주 등장하는 단어를 의미합니다. NLTK는 불용어 리스트를 사용하여 토큰화된 단어에서 불용어를 찾고 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 확인합니다.\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK의 stopwords는 토큰화된 단어가 소문자라고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런도 영어 불용어 리스트를 제공합니다. NLTK의 영어 불용어는 179개이고 사이킷런이 제공하는 영어 불용어는 318개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 179)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "len(ENGLISH_STOP_WORDS), len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 불용어는 frozenset 객체이기 때문에 인덱스를 사용할 수 없습니다. 리스트로 바꾸어 처음 몇 개의 불용어를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never', 'find', 'toward', 'such', 'beforehand']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ENGLISH_STOP_WORDS)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK에서 제공하는 전체 불용어는 다음 주소에서 다운로드할 수 있습니다(https://bit.ly/2vOg4Lu). 안타깝지만 한글 불용어는 제공하지 않습니다. 다음 주소에서 한글 불용어를 참고하세요(https://bit.ly/2Vs05lN, https://bit.ly/2VKOUnF, https://bit.ly/2J912sv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 어간 추출하기\n",
    "NLTK의 PorterStemmer를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenized_words = ['i','am','humbled','by','this','traditional','meeting']\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어간 추출은 단어의 어간을 구분하여 기본 의미를 유지하면서 어미를 제거합니다. 예를 들어 tradition과 traditional은 어간 tradit을 가집니다. 두 단어는 다르지만 동일한 일반적인 개념을 표현합니다. 텍스트 데이터에서 어간을 추출하면 읽기는 힘들어지지만 기본 의미에 가까워지고 샘플 간에 비교하기에 더 좋습니다. 널리 사용되는 포터 어간 추출 알고리즘을 구현한 NLTK의 PorterStemmer는 단어의 어미를 제거하여 어간으로 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 품사 태깅하기\n",
    "사전 훈련된 NLTK의 품사 태깅을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태거를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# 사전 훈련된 품사 태깅을 사용합니다.\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# 품사를 확인합니다.\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력은 단어와 품사 태그로 이루어진 튜플의 리스트입니다. NLTK는 품사 태그를 위해 구문 주석 말뭉치인 Penn Treebank를 사용합니다. \n",
    "\n",
    "[Penn Treebank]\n",
    "- NNP   고유명사, 단수\n",
    "- NN    명사, 단수 또는 불가산 명사\n",
    "- RB    부사\n",
    "- VBD   동사, 과거형\n",
    "- VBG   동사, 동명사 또는 현재분사\n",
    "- JJ    형용사\n",
    "- PRP   인칭 대명사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트가 태깅되면 태그를 사용해 특정 품사를 찾을 수 있습니다. 예를 들어 다음과 같이 모든 명사를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어를 필터링합니다.\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 더 실전 같은 상황은 샘플의 트윗 문장을 각 품사에 따라 특성으로 변환할 때입니다(예를 들어 명사가 있을 경우 1, 그렇지 않으면 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "# 빈 리스트를 만듭니다.\n",
    "tagged_tweets = []\n",
    "\n",
    "# 각 단어와 트윗을 태깅합니다.\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "# 원-핫 인코딩을 사용하여 태그를 특성으로 변환합니다.\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes_를 사용하면 각 특성이 어떤 품사를 나타내는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특별한 주제(예를 들면 의료)에 대한 영어 텍스트가 아니라면 가장 간단한 해결은 사전훈련된 NLTK의 품사 태깅을 사용하는 것입니다. pos_tag의 정확도가 매우 낮다면 NLTK를 사용하여 자신만의 태그 모델을 훈련시킬 수 있습니다. 태그 모델을 훈련하는 데 가장 큰 어려운 점은 각 단어를 태깅한 많은 양의 텍스트 문서가 필요하다는 것입니다. 태깅된 대용량 문서를 만드는 것은 매우 노동 집약적이므로 마지막 수단으로 사용할 수 있습니다.\n",
    "\n",
    "태깅된 문서 데이터가 있고 태그 모델을 훈련하려고 한다면 다음 예를 참조하세요. 이 예제에서 사용하는 문서 데이터는 널리 사용되는 태깅된 텍스트 중 하나인 Brown Corpus 입니다. 여기에서는 backoff n-gram 태그 모델을 사용합니다. n은 한 단어의 품사를 예측하기 위해 고려할 이전 단어의 수입니다. 먼저 TrigramTagger를 사용해 이전 두 단어를 고려하여 만들어보겠습니다. 두 개의 단어가 없다면 BigramTagger를 사용해 이전 한 단어의 품사를 참고합니다. 이것도 실패하면 마지막으로 UnigramTagger를 사용해 그 단어 자체만을 참고합니다. 만들어진 태그 모델의 정확도를 측정하려면 텍스트 데이터를 둘로 나누어 하나는 태그 모델을 훈련하는 데 사용하고 나머지 데이터로는 얼마나 태그를 잘 예측하는지 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 브라운 코퍼스를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# 브라운 코퍼스에서 텍스트를 추출한 다음 문장으로 나눕니다.\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# 4000개의 문장은 훈련용으로 623개는 테스트용으로 나눕니다.\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# 백오프 태그 객체를 만듭니다.\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# 정확도를 확인합니다.\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 품사 태깅을 위한 대표적인 도구는 KoNLPy입니다. KoNLPy는 pip 명령으로 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpyNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.5.0-cp38-cp38-win_amd64.whl (351 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dain\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\dain\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (2.4.7)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy는 기존 tagger들을 손쉽게 사용하도록 도와줍니다. 제공하는 태거로는 Hannanum, Kkma, Komoran, Mecab, Okt가 있습니다. Okt(Open Korean Text)를 사용하여 샘플 문장의 품사를 태깅해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('태양계', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('지금', 'Noun'),\n",
       " ('으로부터', 'Josa'),\n",
       " ('약', 'Noun'),\n",
       " ('46억', 'Number'),\n",
       " ('년', 'Noun'),\n",
       " ('전', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('거대한', 'Adjective'),\n",
       " ('분자', 'Noun'),\n",
       " ('구름', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('일부분', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('중력', 'Noun'),\n",
       " ('붕괴', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('일으키면서', 'Verb'),\n",
       " ('형성', 'Noun'),\n",
       " ('되었다', 'Verb')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = \"태양계는 지금으로부터 약 46억 년 전, 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다\"\n",
    "\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소를 추출하는 morphs 메서드와 명사만을 추출하는 nouns 메서드도 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계',\n",
       " '는',\n",
       " '지금',\n",
       " '으로부터',\n",
       " '약',\n",
       " '46억',\n",
       " '년',\n",
       " '전',\n",
       " ',',\n",
       " '거대한',\n",
       " '분자',\n",
       " '구름',\n",
       " '의',\n",
       " '일부분',\n",
       " '이',\n",
       " '중력',\n",
       " '붕괴',\n",
       " '를',\n",
       " '일으키면서',\n",
       " '형성',\n",
       " '되었다']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계', '지금', '약', '년', '전', '분자', '구름', '일부분', '중력', '붕괴', '형성']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 텍스트를 BoW로 인코딩하기\n",
    "사이킷런의 CountVectorizer를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# BoW 특성 행렬을 만듭니다.\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대량의 텍스트 데이터라면 희소 배열로 출력되는 것이 필수적입니다. 이 예는 간단하기 때문에 toarray 메서드를 사용하여 샘플의 단어 카운트 행렬을 확인해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_feautre_names_out 메서드를 사용해 각 특성에 연결된 단어를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 특성으로 변환하는 방법 중 가장 널리 사용하는 하나는 Bag of Word 모델입니다. BoW 모델은 텍스트 데이터에 있는 고유한 단어마다 하나의 특성을 만듭니다. 이 특성은 각 단어가 샘플에 등장한 횟수를 담고 있습니다. 예를 들면 이 해결에 나온 I love Brazil. Brazil! 문장에서 brazil 단어가 두 번 등장하기 때문에 brazil 특성의 값은 2입니다.\n",
    "\n",
    "여기에서는 예시를 위해 작은 텍스트 데이터를 사용했습니다. 실전에서는 텍스트 데이터의 샘플 하나가 책 한 권 분량일 수 있습니다. BoW 모델이 데이터에 있는 고유한 단어마다 특성 하나를 만들기 때문에 수천 개의 특성을 가진 행렬을 만들 수 있습니다. 이는 이따금 이 행렬이 메모리를 매우 많이 차지할 수 있다는 것을 의미합니다. 다행히 일반적인 BoW 특성 행렬의 특징을 사용하면 데이터 저장 공간을 줄일 수 있습니다.\n",
    "\n",
    "대부분의 단어는 다수의 샘플에 나타나지 않습니다. 따라서 BoW 특성 행렬의 값은 대부분 0입니다. 이런 종류의 행렬을 희소 행렬이라고 부릅니다. 행렬의 모든 원소를 저장하는 대신 0이 아닌 값만 저장하고 나머지 원소는 모두 0이라고 가정할 수 있습니다. 매우 큰 특성 행렬을 다룰 때 메모리를 절약할 수 있습니다. CountVectorizer의 장점 중 하나는 기본적으로 희소 행렬을 출력한다는 것입니다.\n",
    "\n",
    "CountVectorizer는 BoW 특성 행렬을 쉽게 만들 수 있는 편리한 매개변수를 많이 제공합니다. 첫째, 기본적으로 모든 특성은 단어 하나입니다. 이것이 맞지 않을 때가 있습니다. 대신 각 특성을 단어 두 개(2-그램)나 단어 세 개(3-그램)로 만들 수 있습니다. ngram_range 매개변수로 n-그램의 최소와 최대 크기를 지정할 수 있습니다. 예를 들어 (2,3)은 2-그램과 3-그램을 모두 만듭니다. 둘째, stop_words 매개변수를 사용해 내장된 리스트나 사용자가 지정한 리스트에 포함된 유용하지 않은 단어들을 제거할 수 있습니다. 마지막으로 vocabulary 매개변수를 사용해 대상 단어나 구를 제한할 수 있습니다. 예를 들면 국가 이름만 담은 BoW 특성 행렬을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵션을 지정하여 특성 행렬을 만듭니다.\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=['brazil'])\n",
    "\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# 특성 행렬을 확인합니다.\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-그램과 2-그램을 확인합니다.\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트에서 고유한 단어를 추출하여 순서대로 번호를 매긴 것을 어휘 사전이라고 부릅니다. 어휘 사전은 CountVectorizer의 vocabulary_ 속성에 딕셔너리로 저장됩니다. \n",
    "\n",
    "CountVectorizer의 max_df 매개변수는 단어가 등장할 문서의 최대 개수를 지정합니다. 이 매개변수는 너무 자주 등장하는 단어를 제외할 때 사용합니다. 비슷하게 min_df 매개변수는 단어가 등장하는 문서의 최소 개수를 지정합니다. 이 매개변수는 드물게 등장하는 단어를 제외할 때 사용합니다. max_df와 min_df에 0~1 사이의 실숫값을 지정하면 전체 문서 개수에 대한 비율이 됩니다.\n",
    "\n",
    "CountVectorizer가 만드는 어휘 사전 크기를 제한하려면 max_features 매개변수를 지정합니다. 전체 문서에서 빈도순으로 최상위 max_features 개의 단어가 추출됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 단어 중요도에 가중치 부여하기\n",
    "tf-idf(단어 빈도-역문서 빈도)를 사용해 트윗, 영화 리뷰, 연설문 등 하나의 문서에 등장하는 단어의 빈도와 다른 모든 문서에 등장하는 빈도를 비교합니다. 사이킷런의 TfidfVectorizer를 사용하면 간단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# tf-idf 특성 행렬을 만듭니다.\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# tf-idf 특성 행렬을 확인합니다.\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레시피 6.8처럼 이 출력은 희소 행렬입니다. 밀집 배열로 출력하려면 .toarray 매서드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 특성 행렬을 밀집 배열로 확인합니다.\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary_는 각 특성에 해당하는 단어를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 문서에 어떤 단어가 많이 등장할수록 그 문서에 더 중요한 단어일 것입니다. 예를 들어 economy 단어가 자주 나타난다면 경제에 관한 문서라는 증거가 됩니다. 이를 tf(단어 빈도, term frequency)라고 부릅니다.\n",
    "\n",
    "반대로 한 단어가 많은 문서에 나타난다면 이는 어떤 특정 문서에 중요하지 않은 단어라는 뜻입니다. 예를 들어 텍스트 데이터의 모든 문서에 after 단어가 포함되어 있다면 이 단어는 중요하지 않을 것입니다. 이를 df(문서 빈도, document frequency)라고 합니다.\n",
    "\n",
    "이 두 통계치를 연결하여 각 문서가 문서에 얼마나 중요한 단어인지를 점수로 할당할 수 있습니다. 구체적으로 tf를 idf(역문서 빈도)에 곱합니다.\n",
    "\n",
    "$$tf-idf(t,d) = tf(t,d) \\times idf(t)$$\n",
    "\n",
    "여기에서 t는 단어이고 d는 문서입니다. tf와 idf를 계산하는 방법은 여러 가지가 있습니다. 사이킷런에서 tf는 단순히 문서에 나타나는 단어의 등장 횟수입니다. idf는 다음과 같이 계산합니다.\n",
    "\n",
    "$$idf(t) = log\\frac{1+n_d}{1+df(d,t)} + 1$$\n",
    "\n",
    "여기에서 $n_d$는 문서의 개수이고 $df(d,t)$는 단어 $t$의 문서 빈도(즉 단어가 등장하는 문서 개수)입니다.\n",
    "\n",
    "기본적으로 사이킷런은 유클리드 노름(L2 노름)으로 tf-idf 벡터를 정규화합니다. 결괏값이 높을수록 그 문서에서 더 중요한 단어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer의 smooth_idf 매개변수가 기본값(True)일 때 앞서 제시한 공식이 사용됩니다. 로그 안의 분모와 분자에 1을 더하면 모든 단어가 포함된 가상의 문서가 있는 것 같은 효과를 내고 분모가 0이 되는 것을 막아줍니다. 또 모든 문서에 포함된 단어가 있으면 로그값이 0이 되므로, 전체 tf-idf 값이 0이 되는 것을 막기 위해 idf 공식 마지막에 1을 더합니다.\n",
    "\n",
    "smooth_idf=False로 지정하면 분모와 분자에 1을 더하지 않는 공식을 사용합니다.\n",
    "\n",
    "$$idf(t) = log\\frac{n_d}{df(d,t)} + 1$$\n",
    "\n",
    "TfidfVectorizer도 ngram_range, max_df, min_df, max_features 등의 매개변수를 지원합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
