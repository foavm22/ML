{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 정제하기\n",
    "가장 기본적인 정제 방법은 strip, replace, split와 같은 파이썬의 기본 문자열 메서드를 사용하여 텍스트를 바꾸는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"   Interrobang. By Aishwarya Henriette    \",\n",
    "             \"Parking And Going. By Karl Gautier\", \n",
    "             \"    Today Is The night. By Jarek Prakash  \"]\n",
    "\n",
    "# 공백 문자를 제거합니다.\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마침표를 제거합니다.\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 정의 변환 함수를 정의하고 적용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 정규 표현식을 사용하여 강력한 문자열 치환을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 텍스트 데이터는 특성으로 만들기 전에 정제되어야 합니다. 파이썬의 문자열 연산을 사용하는 것이 가장 기본적인 텍스트 정제 방법입니다. 실전에서는 대부분 사용자 정의 함수(예를 들면 capitalizer)를 정의하여 다른 정제 작업과 연결하고 이를 텍스트 데이터에 적용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 HTML 파싱과 정제하기\n",
    "beautiful soup의 다양한 기능을 사용하여 HTML을 파싱할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\dain\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예제 HTML 코드를 만듭니다.\n",
    "html = \"\"\"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "\"\"\"\n",
    "\n",
    "# html을 파싱합니다.\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# \"full_name\" 이름의 클래스를 가진 div를 찾아 텍스트를 출력합니다.\n",
    "soup.find(\"div\", {\"class\" : \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름이 이상하지만 뷰티풀 수프는 HTML 스크래핑을 위한 강력한 파이썬 라이브러리입니다. 일반적으로 뷰티풀 수프는 웹사이트를 스크래핑하는 데 사용합니다. 하지만 HTML에 들어 있는 텍스트 데이터를 추출하는 데 사용할 수도 있습니다. 이 해결에서 소개하는 몇 개의 메서드만으로도 HTML 코드를 파싱하여 원하는 데이터를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 구두점 삭제하기\n",
    "구두점 글자의 딕셔너리를 만들어 translate 메서드에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '1000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '1000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "# 구두점 문자로 이루어진 딕셔너리를 만듭니다.\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# 문자열의 구두점을 삭제합니다.\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate는 속도가 매우 빨라 인기 있는 파이썬 함수입니다. 이 해결은 모든 유니코드 구두점을 키로 하고 값은 None인 punctuation 딕셔너리를 만들었습니다. 그 다음 문자열에서 punctuation에 있는 모든 문자를 None으로 바꾸어 구두점을 삭제하는 효과를 냅니다. 조금 더 읽기 좋은 코드를 사용하는 방법도 있지만 이 방식이 다른 것보다 훨씬 더 빠르다는 장점이 있습니다. \n",
    "\n",
    "구두점도 정보가 있다는 사실을 유념하세요(예를 들어 \"맞아?\"와 \"맞아!\"). 구두점을 삭제하는 것은 특성을 만드는 데 필요악일 경우가 많습니다. 하지만 구두점이 중요한 역할을 한다면 이를 고려해야만 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 텍스트 토큰화하기\n",
    "파이썬의 자연어 처리 툴킷인 NLTK(natural language toolkit)는 단어 토큰화를 비롯해 강력한 텍스트 처리 기능을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dain\\anaconda3\\lib\\site-packages (3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# 단어를 토큰으로 나눕니다.\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장으로 나눌 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화, 특히 단어 토큰화는 텍스트 데이터를 정제한 후 빈번하게 수행하는 작업입니다. 이는 유용한 특성을 만들기 위해 텍스트를 데이터로 변환하는 첫 번째 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 불용어 삭제하기\n",
    "NLTK의 stopwords를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['going', 'store', 'park']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "\n",
    "# 불용어를 적재합니다.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 불용어를 삭제합니다.\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어는 작업 전에 삭제해야 하는 일련의 단어를 의미하기도 하지만 종종 유용한 정보가 거의 없는 매우 자주 등장하는 단어를 의미합니다. NLTK는 불용어 리스트를 사용하여 토큰화된 단어에서 불용어를 찾고 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 확인합니다.\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK의 stopwords는 토큰화된 단어가 소문자라고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런도 영어 불용어 리스트를 제공합니다. NLTK의 영어 불용어는 179개이고 사이킷런이 제공하는 영어 불용어는 318개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 179)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "len(ENGLISH_STOP_WORDS), len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 불용어는 frozenset 객체이기 때문에 인덱스를 사용할 수 없습니다. 리스트로 바꾸어 처음 몇 개의 불용어를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never', 'find', 'toward', 'such', 'beforehand']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ENGLISH_STOP_WORDS)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK에서 제공하는 전체 불용어는 다음 주소에서 다운로드할 수 있습니다(https://bit.ly/2vOg4Lu). 안타깝지만 한글 불용어는 제공하지 않습니다. 다음 주소에서 한글 불용어를 참고하세요(https://bit.ly/2Vs05lN, https://bit.ly/2VKOUnF, https://bit.ly/2J912sv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 어간 추출하기\n",
    "NLTK의 PorterStemmer를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenized_words = ['i','am','humbled','by','this','traditional','meeting']\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어간 추출은 단어의 어간을 구분하여 기본 의미를 유지하면서 어미를 제거합니다. 예를 들어 tradition과 traditional은 어간 tradit을 가집니다. 두 단어는 다르지만 동일한 일반적인 개념을 표현합니다. 텍스트 데이터에서 어간을 추출하면 읽기는 힘들어지지만 기본 의미에 가까워지고 샘플 간에 비교하기에 더 좋습니다. 널리 사용되는 포터 어간 추출 알고리즘을 구현한 NLTK의 PorterStemmer는 단어의 어미를 제거하여 어간으로 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 품사 태깅하기\n",
    "사전 훈련된 NLTK의 품사 태깅을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태거를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# 사전 훈련된 품사 태깅을 사용합니다.\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# 품사를 확인합니다.\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력은 단어와 품사 태그로 이루어진 튜플의 리스트입니다. NLTK는 품사 태그를 위해 구문 주석 말뭉치인 Penn Treebank를 사용합니다. \n",
    "\n",
    "[Penn Treebank]\n",
    "- NNP   고유명사, 단수\n",
    "- NN    명사, 단수 또는 불가산 명사\n",
    "- RB    부사\n",
    "- VBD   동사, 과거형\n",
    "- VBG   동사, 동명사 또는 현재분사\n",
    "- JJ    형용사\n",
    "- PRP   인칭 대명사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트가 태깅되면 태그를 사용해 특정 품사를 찾을 수 있습니다. 예를 들어 다음과 같이 모든 명사를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어를 필터링합니다.\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 더 실전 같은 상황은 샘플의 트윗 문장을 각 품사에 따라 특성으로 변환할 때입니다(예를 들어 명사가 있을 경우 1, 그렇지 않으면 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "# 빈 리스트를 만듭니다.\n",
    "tagged_tweets = []\n",
    "\n",
    "# 각 단어와 트윗을 태깅합니다.\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "# 원-핫 인코딩을 사용하여 태그를 특성으로 변환합니다.\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes_를 사용하면 각 특성이 어떤 품사를 나타내는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특별한 주제(예를 들면 의료)에 대한 영어 텍스트가 아니라면 가장 간단한 해결은 사전훈련된 NLTK의 품사 태깅을 사용하는 것입니다. pos_tag의 정확도가 매우 낮다면 NLTK를 사용하여 자신만의 태그 모델을 훈련시킬 수 있습니다. 태그 모델을 훈련하는 데 가장 큰 어려운 점은 각 단어를 태깅한 많은 양의 텍스트 문서가 필요하다는 것입니다. 태깅된 대용량 문서를 만드는 것은 매우 노동 집약적이므로 마지막 수단으로 사용할 수 있습니다.\n",
    "\n",
    "태깅된 문서 데이터가 있고 태그 모델을 훈련하려고 한다면 다음 예를 참조하세요. 이 예제에서 사용하는 문서 데이터는 널리 사용되는 태깅된 텍스트 중 하나인 Brown Corpus 입니다. 여기에서는 backoff n-gram 태그 모델을 사용합니다. n은 한 단어의 품사를 예측하기 위해 고려할 이전 단어의 수입니다. 먼저 TrigramTagger를 사용해 이전 두 단어를 고려하여 만들어보겠습니다. 두 개의 단어가 없다면 BigramTagger를 사용해 이전 한 단어의 품사를 참고합니다. 이것도 실패하면 마지막으로 UnigramTagger를 사용해 그 단어 자체만을 참고합니다. 만들어진 태그 모델의 정확도를 측정하려면 텍스트 데이터를 둘로 나누어 하나는 태그 모델을 훈련하는 데 사용하고 나머지 데이터로는 얼마나 태그를 잘 예측하는지 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 브라운 코퍼스를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# 브라운 코퍼스에서 텍스트를 추출한 다음 문장으로 나눕니다.\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# 4000개의 문장은 훈련용으로 623개는 테스트용으로 나눕니다.\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# 백오프 태그 객체를 만듭니다.\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# 정확도를 확인합니다.\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 품사 태깅을 위한 대표적인 도구는 KoNLPy입니다. KoNLPy는 pip 명령으로 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpyNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.5.0-cp38-cp38-win_amd64.whl (351 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dain\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\dain\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dain\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (2.4.7)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy는 기존 tagger들을 손쉽게 사용하도록 도와줍니다. 제공하는 태거로는 Hannanum, Kkma, Komoran, Mecab, Okt가 있습니다. Okt(Open Korean Text)를 사용하여 샘플 문장의 품사를 태깅해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('태양계', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('지금', 'Noun'),\n",
       " ('으로부터', 'Josa'),\n",
       " ('약', 'Noun'),\n",
       " ('46억', 'Number'),\n",
       " ('년', 'Noun'),\n",
       " ('전', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('거대한', 'Adjective'),\n",
       " ('분자', 'Noun'),\n",
       " ('구름', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('일부분', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('중력', 'Noun'),\n",
       " ('붕괴', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('일으키면서', 'Verb'),\n",
       " ('형성', 'Noun'),\n",
       " ('되었다', 'Verb')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = \"태양계는 지금으로부터 약 46억 년 전, 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다\"\n",
    "\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소를 추출하는 morphs 메서드와 명사만을 추출하는 nouns 메서드도 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계',\n",
       " '는',\n",
       " '지금',\n",
       " '으로부터',\n",
       " '약',\n",
       " '46억',\n",
       " '년',\n",
       " '전',\n",
       " ',',\n",
       " '거대한',\n",
       " '분자',\n",
       " '구름',\n",
       " '의',\n",
       " '일부분',\n",
       " '이',\n",
       " '중력',\n",
       " '붕괴',\n",
       " '를',\n",
       " '일으키면서',\n",
       " '형성',\n",
       " '되었다']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계', '지금', '약', '년', '전', '분자', '구름', '일부분', '중력', '붕괴', '형성']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.nouns(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
