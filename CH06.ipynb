{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 정제하기\n",
    "가장 기본적인 정제 방법은 strip, replace, split와 같은 파이썬의 기본 문자열 메서드를 사용하여 텍스트를 바꾸는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"   Interrobang. By Aishwarya Henriette    \",\n",
    "             \"Parking And Going. By Karl Gautier\", \n",
    "             \"    Today Is The night. By Jarek Prakash  \"]\n",
    "\n",
    "# 공백 문자를 제거합니다.\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마침표를 제거합니다.\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 정의 변환 함수를 정의하고 적용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 정규 표현식을 사용하여 강력한 문자열 치환을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 텍스트 데이터는 특성으로 만들기 전에 정제되어야 합니다. 파이썬의 문자열 연산을 사용하는 것이 가장 기본적인 텍스트 정제 방법입니다. 실전에서는 대부분 사용자 정의 함수(예를 들면 capitalizer)를 정의하여 다른 정제 작업과 연결하고 이를 텍스트 데이터에 적용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 HTML 파싱과 정제하기\n",
    "beautiful soup의 다양한 기능을 사용하여 HTML을 파싱할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\dain\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\dain\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예제 HTML 코드를 만듭니다.\n",
    "html = \"\"\"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "\"\"\"\n",
    "\n",
    "# html을 파싱합니다.\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# \"full_name\" 이름의 클래스를 가진 div를 찾아 텍스트를 출력합니다.\n",
    "soup.find(\"div\", {\"class\" : \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름이 이상하지만 뷰티풀 수프는 HTML 스크래핑을 위한 강력한 파이썬 라이브러리입니다. 일반적으로 뷰티풀 수프는 웹사이트를 스크래핑하는 데 사용합니다. 하지만 HTML에 들어 있는 텍스트 데이터를 추출하는 데 사용할 수도 있습니다. 이 해결에서 소개하는 몇 개의 메서드만으로도 HTML 코드를 파싱하여 원하는 데이터를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 구두점 삭제하기\n",
    "구두점 글자의 딕셔너리를 만들어 translate 메서드에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '1000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '1000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "# 구두점 문자로 이루어진 딕셔너리를 만듭니다.\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# 문자열의 구두점을 삭제합니다.\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate는 속도가 매우 빨라 인기 있는 파이썬 함수입니다. 이 해결은 모든 유니코드 구두점을 키로 하고 값은 None인 punctuation 딕셔너리를 만들었습니다. 그 다음 문자열에서 punctuation에 있는 모든 문자를 None으로 바꾸어 구두점을 삭제하는 효과를 냅니다. 조금 더 읽기 좋은 코드를 사용하는 방법도 있지만 이 방식이 다른 것보다 훨씬 더 빠르다는 장점이 있습니다. \n",
    "\n",
    "구두점도 정보가 있다는 사실을 유념하세요(예를 들어 \"맞아?\"와 \"맞아!\"). 구두점을 삭제하는 것은 특성을 만드는 데 필요악일 경우가 많습니다. 하지만 구두점이 중요한 역할을 한다면 이를 고려해야만 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 텍스트 토큰화하기\n",
    "파이썬의 자연어 처리 툴킷인 NLTK(natural language toolkit)는 단어 토큰화를 비롯해 강력한 텍스트 처리 기능을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dain\\anaconda3\\lib\\site-packages (3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\dain\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 텍스트를 만듭니다.\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# 단어를 토큰으로 나눕니다.\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장으로 나눌 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화, 특히 단어 토큰화는 텍스트 데이터를 정제한 후 빈번하게 수행하는 작업입니다. 이는 유용한 특성을 만들기 위해 텍스트를 데이터로 변환하는 첫 번째 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 불용어 삭제하기\n",
    "NLTK의 stopwords를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['going', 'store', 'park']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 데이터를 다운로드합니다.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 라이브러리를 임포트합니다.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "\n",
    "# 불용어를 적재합니다.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 불용어를 삭제합니다.\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어는 작업 전에 삭제해야 하는 일련의 단어를 의미하기도 하지만 종종 유용한 정보가 거의 없는 매우 자주 등장하는 단어를 의미합니다. NLTK는 불용어 리스트를 사용하여 토큰화된 단어에서 불용어를 찾고 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 확인합니다.\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK의 stopwords는 토큰화된 단어가 소문자라고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런도 영어 불용어 리스트를 제공합니다. NLTK의 영어 불용어는 179개이고 사이킷런이 제공하는 영어 불용어는 318개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 179)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "len(ENGLISH_STOP_WORDS), len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 불용어는 frozenset 객체이기 때문에 인덱스를 사용할 수 없습니다. 리스트로 바꾸어 처음 몇 개의 불용어를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never', 'find', 'toward', 'such', 'beforehand']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ENGLISH_STOP_WORDS)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK에서 제공하는 전체 불용어는 다음 주소에서 다운로드할 수 있습니다(https://bit.ly/2vOg4Lu). 안타깝지만 한글 불용어는 제공하지 않습니다. 다음 주소에서 한글 불용어를 참고하세요(https://bit.ly/2Vs05lN, https://bit.ly/2VKOUnF, https://bit.ly/2J912sv)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
