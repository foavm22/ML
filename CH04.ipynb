{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23, Jan, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 특성 스케일 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "## 특성을 만듭니다.\n",
    "feature = np.array([[-500.5],\n",
    "                   [-100.1],\n",
    "                   [0],\n",
    "                   [100.1],\n",
    "                   [900.9]])\n",
    "\n",
    "## 스케일러 객체를 만듭니다.\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "## 특성의 스케일을 변환합니다.\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "## 특성을 출력합니다.\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스케일 조정은 머신러닝에서 흔한 전처리 작업입니다. 이 책에서 설명하는 대부분의 알고리즘은 모든 특성이 동일한 스케일을 가지고 있다고 가정합니다. 일반적으로 0 ~ 1이나 -1~1 사이입니다. 스케일 조정 기법은 여러 가지인데, 가장 간단한 방법은 최소-최대 스케일링(min-max scaling)입니다. 최소-최대 스케일링은 특성의 최솟값과 최댓값을 사용하여 일정 범위 안으로 값을 조정합니다. 구체적으로 최소-최대 스케일링은 다음과 같이 계산됩니다.\n",
    "\n",
    "$$x_{i}^{'} = \\frac{x_i-min(x)}{max(x)-min(x)}$$\n",
    "\n",
    "$x$는 특성 벡터이고 $x_i$는 특성 $x$의 개별 원소입니다. $x_{i}^{'}$ 는 스케일이 바뀐 원소입니다.이 예에서는 출력 배열의 스케일이\n",
    "0에서 1 사이로 성공적으로 바뀌었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 MinMaxScaler는 특성 스케일을 위해 두 가지 방법을 제공합니다. 첫 번째로 fit 메서드를 사용해 특성의 최솟값과 최댓값을 계산한 다음 transform 메서드로 특성의 스케일을 조정합니다. 두 번째로 fit_transform 메서드로 두 연산을 한번에 처리합니다. 이 둘 사이에 계산 상의 차이는 없습니다. 동일한 변환을 다른 데이터 세트에 적용하려면 fit 메서드와 transform 메서드를 따로 호출해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.8],\n",
       "       [1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 세트와 테스트 세트의 스케일을 따로 조정하면 안 됩니다 예를 들면 훈련 세트의 스케일을 조정하고자 구한 최솟값과 최댓값을 사용하여\n",
    "# 테스트 세트를 변환해야 합니다. 간단한 예를 통해 이유를 알아보죠,\n",
    "\n",
    "# 해결에 있는 샘플 중 처음 세 개를 훈련 세트, 나머지 두 개를 테스트 세트라고 가정해보겠습니다. \n",
    "# 먼저 두 세트를 독립적으로 각각 변환합니다.\n",
    "\n",
    "## 훈련 세트를 변환합니다.\n",
    "preprocessing.MinMaxScaler().fit_transform(feature[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 테스트 세트를 변환합니다.\n",
    "preprocessing.MinMaxScaler().fit_transform(feature[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.8],\n",
       "       [1. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 세트와 테스트 세트를 각각 변환하면 서로 다른 비율로 데이터를 변환합니다. 훈련 세트에 있는 0과 테스트 세트에 있는 900.1가 모두\n",
    "# 1로 바뀌었습니다. 데이터가 다른 스케일로 변환되면 훈련 세트에서 학습한 모델을 테스트 세트에서 사용할 수 없습니다.\n",
    "\n",
    "# 이번에는 훈련 세트에서 학습한 변환기로 테스트를 학습해보죠.\n",
    "\n",
    "## 훈련 세트로 변환기를 학습합니다.\n",
    "scaler = preprocessing.MinMaxScaler().fit(feature[:3])\n",
    "scaler.transform(feature[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2],\n",
       "       [2.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 훈련 세트에서 학습한 변환기로 테스트 세트를 변환합니다.\n",
    "scaler.transform(feature[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 특성을 표준화하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## 특성을 만듭니다.\n",
    "x = np.array([[-1000.1],\n",
    "             [-200.2],\n",
    "             [500.5],\n",
    "             [600.6],\n",
    "             [9000.9]])\n",
    "\n",
    "## 변환기 객체를 만듭니다.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## 특성을 변환합니다.\n",
    "standardized = scaler.fit_transform(x)\n",
    "\n",
    "## 특성을 출력합니다.\n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레시피 4.1에서 설명한 최소-최대 스케일링과 함께 특성을 표준 정규분포로 근사하는 스케일링 방식이 자주 쓰입니다. \n",
    "이 방식은 표준화를 사용하여 데이터의 평균 $\\bar{x}$가 0이고 표준편차 $\\sigma$가 1이 되도록 변환합니다.\n",
    "구체적으로 각 특성은 다음과 같이 변환됩니다.\n",
    "$$ x_{i}^{'} = \\frac{x_{i} - \\bar{x}}{\\sigma} $$\n",
    "\n",
    "$x_{i}^{'}$ 는 $x_{i}$의 표준화된 형태입니다. 변환된 특성은 원본 값이 특성 평균에서 몇 표준편차만큼 떨어져 있는지로 표현합니다.\n",
    "(통계학에서는 z-점수라고도 부릅니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균: 0\n",
      "표준편차: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 표준화는 머신러닝의 일반적인 전처리 단계에서 사용할 수 있는 믿을 만한 스케일링 방법입니다. \n",
    "# 제 경험으로 비추어보았을 때 최소-최대 스케일링보다 많이 쓰입니다. 하지만 학습 알고리즘에 의존적입니다. \n",
    "# 예를 들어 주성분 분석(principal component analysis)은 표준화가 잘 맞지만 \n",
    "# 신경망(neural network)에는 최소-최대 스케일링을 종종 권장합니다. \n",
    "# 일반적으로 다른 방법을 사용할 특별한 이유가 없다면 기본으로 표준화를 권장합니다.\n",
    "\n",
    "# 해결의 출력 결과에서 평균과 표준편차를 구해 표준화의 효과를 확인해보겠습니다.\n",
    "\n",
    "print(\"평균:\", round(standardized.mean()))\n",
    "print(\"표준편차:\", standardized.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터에 이상치가 많다면 특성의 평균과 표준편차에 영향을 미치기 때문에 표준화에 부정적인 효과를 끼칩니다. 이런 경우에는 중간값과 사분위 범위를 사용하여 특성의 스케일을 조정하는 것이 좋습니다. 사이킷런의 RobustScaler가 이런 방법을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 변환기 객체를 만듭니다.\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "## 특성을 변환합니다.\n",
    "robust_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RobustScaler는 데이터에서 중간값을 빼고 IQR로 나눕니다.\n",
    "\n",
    "interquatile_range = x[3] -x[1]\n",
    "(x- np.median(x)) / interquatile_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dain\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2367: UserWarning: n_quantiles (1000) is greater than the total number of samples (5). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\"n_quantiles (%s) is greater than the total number \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.25],\n",
       "       [0.5 ],\n",
       "       [0.75],\n",
       "       [1.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QuantileTransformer는 훈련 데이터를 1,000개 분위로 나누어 0~1 사이에 고르게 분포시킴으로써 이상치로 인한 영향을 줄입니다.\n",
    "# 예를 들어 해결에 나온 특성 x는 다섯 개의 샘플을 가지고 있으므로 0%, 25%, 50%, 75%, 100%의 위치에 할당합니다.\n",
    "\n",
    "preprocessing.QuantileTransformer().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 정규화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "features = np.array([[0.5, 0.5],\n",
    "                    [1.1, 3.4],\n",
    "                    [1.5, 20.2],\n",
    "                    [1.63, 34.4],\n",
    "                    [10.9, 3.3]])\n",
    "\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "\n",
    "normalizer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 스케일링 방법이(예를 들면 최소-최대 스케일링과 표준화와 같이) 특성별로 적용되지만 샘플별로 스케일을 바꿀 수도 있습니다.Normalizer는 단위 노름(길이의 합이 1입니다)이 되도록 개별 샘플의 값을 변환합니다. 이런 종류의 스케일링은 (예를 들어 각 단어나 n개의)단어 그룹이 특성인 텍스트 분류와 같이) 유사한 특성이 많을 때 종종 사용합니다.\n",
    "\n",
    "Normalizer는 세 가지 노름 옵션을 제공합니다. 그 중 L2 노름이라고도 부르는 유클리드 노름이 기본값입니다.\n",
    "$$ \\left\\|x \\right\\|_2 = \\sqrt{x_{1}^{2}+x_{2}^{2}+\\cdots +x_{n}^{2}} $$\n",
    "\n",
    "$x$는 개별 샘플이고 $x_n$은 샘플의 $n$번째 특성값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_l2_norm = Normalizer(norm=\"l2\").transform(features)\n",
    "\n",
    "features_l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨해튼 노름(L2)을 지정할 수도 있습니다.\n",
    "\n",
    "$$ \\left\\|x \\right\\|_1 = \\sum_{i=1}^{n}\\left|x_i\\right| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.06912442, 0.93087558],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n",
    "\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 샘플값의 합: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 직관적으로 생각했을 때 L2 노름은 뉴욕의 두 지점 사이를 잇는 직선 거리로 볼 수있습니다. \n",
    "# L1 노름은 사람이 도로를 따라 걷는 것과 같습니다. 그래서 '맨해튼 노름' 또는 '택시 노름'으로 부릅니다.\n",
    "\n",
    "# norm=\"l1\"은 각 샘플 특성값의 합을 1로 만듭니다. 실제 이런 성질이 가끔 필요할 때가 있습니다.\n",
    "\n",
    "print(\"첫 번째 샘플값의 합:\",\n",
    "     features_l1_norm[0,0] + features_l1_norm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.06912442, 0.93087558],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizer는 행 단위로 변환되므로 fit 메서드는 아무런 작업을 수행하지 않습니다. 이런 이유로 해결의 코드처럼 \n",
    "# 바로 transform 메서드를 사용할 수 있습니다. 'l1'과 'l2' 옵션의 변환은 각 행의 L1 노름과 L2 노름을 구해 나누는 것입니다.\n",
    "\n",
    "## L1 노름을 사용한 변환.\n",
    "## 각 행(axis=1)을 합한 결과가 2차원 배열로 유지되도록\n",
    "## keepdims를 True로 설정합니다.\n",
    "\n",
    "features / np.sum(np.abs(features), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## L2 노름을 사용한 변환\n",
    "\n",
    "features / np.sqrt(np.sum(np.square(features), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        ],\n",
       "       [0.32352941, 1.        ],\n",
       "       [0.07425743, 1.        ],\n",
       "       [0.04738372, 1.        ],\n",
       "       [1.        , 0.30275229]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizer의 norm 매개변수에 지정할 수 있는 다른 한 가지 옵션은 'max' 입니다. 이 옵션은 단순히 각 행의 최댓값으로 행의 값을 나눕니다.\n",
    "\n",
    "Normalizer(norm=\"max\").transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 다항 특성과 교차항 특성 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "features = np.array([[2, 3],\n",
    "                     [2, 3],\n",
    "                     [2, 3]])\n",
    "\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "polynomial_interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree 매개변수가 다항식의 최대 차수를 결정합니다. 예를 들어 degree=2는 2제곱까지 새로운 특성을 만듭니다.\n",
    "\n",
    "$$x_1, x_2, x_1^{2}, x_{2}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree=3은 2제곱과 3제곱까지 새로운 특성을 만듭니다.\n",
    "$$x_1, x_2, x_1^2, x_2^2, x_{1}^{3}, x_{2}^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 PolynomialFeatures는 교차항을 포함합니다.\n",
    "$$x_1x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6.],\n",
       "       [2., 3., 6.],\n",
       "       [2., 3., 6.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interaction_only를 True로 지정하면 교차항 특성만 만들 수 있습니다.\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2,\n",
    "                                 interaction_only=True, include_bias=False)\n",
    "\n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특성과 타깃 사이에 비선형 관계가 있다는 가정을 추가할 때 다항 특성을 종종 만듭니다. 예를 들면 주요 질병에 걸릴 확률에 나이가 미치는 영향은 일정한 상숫값이 아니고 나이가 증가함에 따라 같이 증가한다는 의심을 할 수 있습니다. 특성 $x$에 변동 효과를 주입하기 위해서 고차항 트겅을 만들 수 있습니다($x^2$, $x^3$ 등).\n",
    "\n",
    "또한 한 특성의 효과가 다른 특성에 의존하는 경우를 자주 만나게 됩니다. 간단한 예는 커피가 달달한지 예측하는 문제입니다. 여기에는 두 개의 특성이 있습니다. 1) 커피를 저었는지 여부 2) 설탕을 넣었는지 여부입니다. 각 특성은 독립적으로는 커피의 당도를 예측하지 못하지만 이 둘의 조합은 가능합니다. 즉 커피에 설탕을 넣고 저었을 때에만 커피가 달달합니다. 타깃(달달함)에 대한 각 특성의 영향은 서로에게 종속적입니다. 개별 특성을 곱한 교차항을 특성에 추가하여 이런 관계를 인코딩할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 6., 9.],\n",
       "       [1., 2., 3., 4., 6., 9.],\n",
       "       [1., 2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include_bias 매개변수의 기본값은 True입니다. 이 설정은 변환된 특성에 상수항 1을 추가합니다.\n",
    "\n",
    "polynomial_bias = PolynomialFeatures(degree=2,\n",
    "                                     include_bias=True).fit(features)\n",
    "\n",
    "polynomial_bias.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_feature_names 메서드는 특성 변환 식을 이름으로 반환합니다.\n",
    "\n",
    "polynomial_bias.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
